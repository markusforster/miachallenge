{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7482ea16-4aa1-4d5f-840d-2b82fca45a99",
   "metadata": {},
   "source": [
    "# Compound challenge\n",
    "The idea is to generate a trainingset from lowercase version of all all permutation of sub-words from the given compound nouns. Train a model for classification. <br>\n",
    "In the API we eliminate stopwords from the input, lowercase and remove whitespaces. The encoded string is then processed by the classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5452221-8a72-4b90-a5cc-9761388d1cb4",
   "metadata": {},
   "source": [
    "## Load pandas library for input of icd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45a1a34-5b77-4d0a-ace5-e68d68ee9f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt\n",
    "# !pip install -U tensorflow-gpu\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddfc9f3-ae17-4d6a-8760-6e449dd5c8da",
   "metadata": {},
   "source": [
    "### Load icd data from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac1b91-c01e-43c0-b86d-f67248a5b286",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('icd.csv', delimiter=';',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b64a808-6619-4c46-a047-0ef2071544c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8fc8b3-b33e-4366-ba04-5b273ee8b3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_no = {x[1]:i for i,x in enumerate(df.iloc())} \n",
    "print(label_to_no)\n",
    "no_to_label = {i:x[1] for i,x in enumerate(df.iloc())}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f291c45f-cee5-4523-a08a-bb9cbd38921b",
   "metadata": {},
   "source": [
    "## Load libraries to split German compounds and for creation of permutations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c3b605-5737-4679-b27d-a67f99db8405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "from compound_split import char_split\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937851f6-ac53-4ab5-90a5-f744a5cb5a72",
   "metadata": {},
   "source": [
    "### 1) Load German language model for spacy\n",
    "### 2) Split each Compound into the 5 most probable sub-words\n",
    "### 3) Generate a dataset containing all permutations of the splitted sub-words with the icd label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbffecf-18f7-40c9-b549-8a05253636d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "X = {}\n",
    "\n",
    "for i in df.iloc():\n",
    "    ## Generate the first 5 compund splits\n",
    "    text = [x for x in [' '.join(w[1:]) for w in char_split.split_compound(i[0])][:5]]\n",
    "    label = i[1]\n",
    "    for t in text:\n",
    "        setofwords=[x.lemma_.lower() for x in nlp(t) if not x.is_stop]\n",
    "        perms = list(permutations(setofwords))\n",
    "        sperms = list(''.join(p) for p in perms)\n",
    "        if \"text\" in X:\n",
    "            X[\"text\"] += [str(sp) for sp in sperms]\n",
    "            X[\"label\"] += [int(label_to_no[label]) for sp in sperms]\n",
    "        else:\n",
    "            X[\"text\"] = [str(sp) for sp in sperms]\n",
    "            X[\"label\"] = [int(label_to_no[label]) for sp in sperms]\n",
    "\n",
    "X_data = Dataset.from_dict(X) # Dataset.from_dict(X)\n",
    "print(X_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f4659f-86f4-48bc-bb50-7f9b34ddf1c5",
   "metadata": {},
   "source": [
    "### Load transformer libary from huggingface to use distilbert (German) word model as encoder and decoder (with logits on the last layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091fcf5d-0df1-4c9f-be1f-266ed8ba810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2079810-94d9-41a4-856c-01a3ca9e2c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-german-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b897ccb-46e2-4f1c-8586-b773bc8bcf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(x):\n",
    "    return tokenizer(x[\"text\"], truncation=True, max_length=10, padding='max_length')\n",
    "\n",
    "X_train = X_data.map(preprocess_function,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f06a26-db5a-4c86-a8ef-eb3534cee691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-german-cased\", num_labels=7)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d762ca3-67ec-4b45-a602-5ec4c9bddc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c09df3-2f37-4b94-a449-c397c99d7f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=X_train,\n",
    "    eval_dataset=X_train,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1db1e7-ff23-40c5-922a-61fdfb077485",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b99f1a-c1b9-4fa1-9695-91c6c83ce9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "token = tokenizer(\"riss der arterie im rechten arm\", max_length=10, padding='max_length',truncation=True,return_tensors='pt')\n",
    "labels = torch.tensor([1]).unsqueeze(0)\n",
    "labels = labels.to(device='cuda')\n",
    "token = token.to(device='cuda')\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb55b480-00c1-46fe-87bb-2e7a575aee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "output = model(**token,labels=labels)\n",
    "print(output)\n",
    "print(torch.softmax(output.logits,1))\n",
    "no_to_label[(np.argmax(output.logits.detach().cpu().numpy()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a04a850-65e6-4845-a097-4558d3ae7334",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'./icd.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9b1970-1a8f-40ef-a50d-d5e8824481cc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
